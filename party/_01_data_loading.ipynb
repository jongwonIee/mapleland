{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e67ee6e9-e19e-4bc9-8326-9e843a6bd2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "#https://discord.com/channels/1134059900666916935/1283610000484208670\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc58157e-c2e3-4ccc-bc8d-24775d41b3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output folders\n",
    "input_folder = \"data/txt\"\n",
    "output_folder = \"data/output\"\n",
    "file_paths = [os.path.join(input_folder, file) for file in os.listdir(input_folder) if file.endswith(\".txt\")]\n",
    "\n",
    "# Ensure the output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Job dictionary and spec ranges\n",
    "job_dict = {\n",
    "    \"허\": \"허밋\", \"시프\": \"시프\", \"썬\": \"썬콜\", \"불독\": \"불독\",\n",
    "    \"프\": \"프리스트\", \"레\": \"레인저\", \"저\": \"저격수\",\n",
    "    \"용\": \"용기사\", \"크\": \"크루세이더\", \"나\": \"나이트\"\n",
    "}\n",
    "job_spec_ranges = {\n",
    "    \"용\": (3000, 9000), \"크\": (2000, 9000), \"나\": (2000, 9000),\n",
    "    \"허\": (1500, 4000), \"시프\": (1500, 5000), \"썬\": (500, 1200),\n",
    "    \"불독\": (500, 1200), \"프\": (500, 1200), \"레\": (2000, 9000),\n",
    "    \"저\": (2000, 9000)\n",
    "}\n",
    "level_min, level_max = 80, 190\n",
    "\n",
    "# Regex patterns\n",
    "time_pattern = r\"(오전|오후) \\d{1,2}:\\d{2}\"\n",
    "job_pattern = r\"(\\d{2,3})\\s?(\" + \"|\".join(job_dict.keys()) + r\")\"\n",
    "spec_pattern = r\"(\\d{3,4})\"\n",
    "map_pattern = r\"(망용둥|위둥|남둥|큰둥|와협|블와둥|협동|레와둥|붉켄|검켄|푸켄|불어전|물어전|오징어|깊바협|망둥쩔|듀파|듀미굴|갈림길|산양|하둥)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e6e0ade-234a-425c-afee-14e3b3707f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_level_and_spec(text, job_start, job_end, level_range, spec_range):\n",
    "    left_text = text[job_start-5 : job_start+5]\n",
    "    right_text = text[job_end+1 : job_end + 10]\n",
    "    \n",
    "    level, spec = None, None\n",
    "\n",
    "    # Search for level in the left_text\n",
    "    for match in re.finditer(r\"\\d{2,3}\", left_text[::-1]):  # Reverse for easier parsing\n",
    "        num = int(match.group()[::-1])  # Reverse back the number\n",
    "        if level_range[0] <= num <= level_range[1]:\n",
    "            level = num\n",
    "            break\n",
    "\n",
    "    # Search for spec in the right_text\n",
    "    for match in re.finditer(r\"\\d{3,4}\", right_text):\n",
    "        num = int(match.group())\n",
    "        if spec_range[0] <= num <= spec_range[1]:\n",
    "            spec = num\n",
    "            break\n",
    "\n",
    "    return level, spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba45f84-7185-4aa2-8455-69abc29fe74d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cca60f2-0bd3-4ce8-b269-e7ab6eeffafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_initial_data(file_path):\n",
    "    columns = [\"party_id\", \"time\", \"level\", \"job\", \"spec\", \"map\", \"date\", \"valid\", \"valid_spec\"]\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "\n",
    "    # Extract date from filename\n",
    "    month = file_path[-8:-6]\n",
    "    day = file_path[-6:-4]\n",
    "    date_from_file = f\"2024-{month}-{day}\"\n",
    "    \n",
    "    party_id = 1\n",
    "    \n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = file.read()\n",
    "\n",
    "    blocks = data.split(\"오늘\")\n",
    "    for block in blocks[1:]:\n",
    "        time_match = re.search(time_pattern, block)\n",
    "        time = time_match.group(0) if time_match else None\n",
    "        map_match = re.search(map_pattern, block)\n",
    "        map_name = map_match.group(0) if map_match else None\n",
    "\n",
    "        job_matches = list(re.finditer(job_pattern, block))\n",
    "        for job_match in job_matches:\n",
    "            job_short = job_match.group(2)\n",
    "            job_full = job_dict.get(job_short, \"Unknown\")\n",
    "\n",
    "            level, spec = extract_level_and_spec(\n",
    "                block, job_match.start(), job_match.end(),\n",
    "                (level_min, level_max), job_spec_ranges.get(job_short, (None, None))\n",
    "            )\n",
    "            \n",
    "            valid = map_name is not None and job_full != \"Unknown\"\n",
    "\n",
    "            # print(level, job_full, spec)\n",
    "            \n",
    "            # Append new row to DataFrame\n",
    "            df = pd.concat([\n",
    "                df,\n",
    "                pd.DataFrame([{\n",
    "                    \"party_id\": party_id,\n",
    "                    \"time\": time,\n",
    "                    \"level\": level,\n",
    "                    \"job\": job_full,\n",
    "                    \"spec\": spec,\n",
    "                    \"map\": map_name,\n",
    "                    \"date\": date_from_file,\n",
    "                    \"valid\": valid,\n",
    "                    \"valid_spec\": None\n",
    "                }])\n",
    "            ], ignore_index=True)\n",
    "\n",
    "        party_id += 1\n",
    "\n",
    "    # Remove if level is NaN\n",
    "    df = df.dropna(subset=['level'])\n",
    "    # Remove duplicates\n",
    "    df = df.drop_duplicates(subset=[\"map\", \"level\", \"job\", \"spec\"], keep=\"first\")\n",
    "\n",
    "    #스펙 결측치 처리\n",
    "    # Step 1: Calculate spec_by_level, handling NaN values for spec and level\n",
    "    df[\"spec_by_level\"] = df.apply(\n",
    "        lambda row: row[\"spec\"] / row[\"level\"] if pd.notna(row[\"spec\"]) and pd.notna(row[\"level\"]) else None,\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Step 2: Calculate average spec_by_level by job\n",
    "    avg_spec_by_level = df.groupby(\"job\")[\"spec_by_level\"].mean().rename(\"avg_spec_by_level\")\n",
    "    \n",
    "    # Step 3: Merge the avg_spec_by_level back into the dataframe\n",
    "    df = df.merge(avg_spec_by_level, on=\"job\", how=\"left\")\n",
    "    \n",
    "    # Step 4: Fill NaN in spec_by_level with the average spec_by_level for the job\n",
    "    df[\"spec_by_level\"].fillna(df[\"avg_spec_by_level\"], inplace=True)\n",
    "    \n",
    "    # Step 5: Determine spec_valid based on 20% margin from avg_spec_by_level\n",
    "    def is_spec_valid(row):\n",
    "        if pd.notna(row[\"spec\"]):  # Only proceed if spec is not NaN\n",
    "            if pd.notna(row[\"spec_by_level\"]) and pd.notna(row[\"avg_spec_by_level\"]):\n",
    "                lower_bound = 0.8 * row[\"avg_spec_by_level\"]\n",
    "                upper_bound = 1.2 * row[\"avg_spec_by_level\"]\n",
    "                return lower_bound <= row[\"spec_by_level\"] <= upper_bound\n",
    "        return False  # Return False if spec is NaN or not within the range\n",
    "    \n",
    "    # Step 6: Apply the is_spec_valid function to the dataframe\n",
    "    df[\"valid_spec\"] = df.apply(is_spec_valid, axis=1)\n",
    "    \n",
    "    # Step 7: Create spec_filled: if spec is NaN, fill with level * avg_spec_by_level, else keep spec as is\n",
    "    df[\"spec_filled\"] = df.apply(\n",
    "        lambda row: row[\"spec\"] if pd.notna(row[\"spec\"]) else (\n",
    "            row[\"level\"] * row[\"avg_spec_by_level\"] if pd.notna(row[\"level\"]) and pd.notna(row[\"avg_spec_by_level\"]) else None\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Step 8: Calculate the average distance between spec and avg_spec_by_level for each job\n",
    "    df[\"spec_distance\"] = df.apply(\n",
    "        lambda row: abs(row[\"spec\"] - row[\"avg_spec_by_level\"]) if pd.notna(row[\"spec\"]) and pd.notna(row[\"avg_spec_by_level\"]) else None,\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Step 9: Calculate the average distance for each job\n",
    "    job_avg_distances = df.groupby(\"job\")[\"spec_distance\"].mean().rename(\"avg_distance_per_job\")\n",
    "    \n",
    "    # Step 10: Merge the average distance per job back into the dataframe\n",
    "    df = df.merge(job_avg_distances, on=\"job\", how=\"left\")\n",
    "\n",
    "    JITTER_PERCENTAGE = 0.03\n",
    "    # Step 11: Apply jitter based on the average distance per job\n",
    "    # Apply jitter ONLY if spec_filled is created (i.e., spec is NaN)\n",
    "    df[\"spec_filled\"] = df.apply(\n",
    "    lambda row: row[\"spec_filled\"] + np.random.uniform(-row[\"spec_filled\"] * JITTER_PERCENTAGE, row[\"spec_filled\"] * JITTER_PERCENTAGE)\n",
    "    if pd.isna(row[\"spec\"]) and pd.notna(row[\"spec_filled\"]) else row[\"spec_filled\"],  # Apply jitter only when spec is missing\n",
    "    axis=1\n",
    "    )\n",
    "\n",
    "    df.drop(columns=['spec_distance', 'avg_distance_per_job'], inplace=True)\n",
    "    \n",
    "    output_file = os.path.join(output_folder, os.path.basename(file_path).replace('.txt', '.csv'))\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Data saved as {output_file}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3112bdb-12d8-4c51-866a-beae9b355bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(df, file_path):\n",
    "    \"\"\"\n",
    "    Filters and processes data to save only rows where valid and valid_spec are True.\n",
    "    \"\"\"\n",
    "    # Filter rows where both 'valid' and 'valid_spec' are True\n",
    "    processed_df = df[(df[\"valid\"]) & (df[\"valid_spec\"])]\n",
    "\n",
    "    # Save the processed DataFrame to a CSV file\n",
    "    processed_output_file = os.path.join(output_folder, os.path.basename(file_path).replace('.txt', '_processed.csv'))\n",
    "    processed_df.to_csv(processed_output_file, index=False)\n",
    "\n",
    "    print(f\"Processed valid data saved as {processed_output_file}\")\n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49d9b780-bccf-45b7-a2fa-7dea319e2664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_level_outlier(df, level_percentile=99.99):\n",
    "    # Filter out rows where 'level' is NaN\n",
    "    df_level_not_na = df[df['level'].notna()]\n",
    "\n",
    "    # Calculate the upper and lower level thresholds\n",
    "    upper_level_threshold = np.percentile(df_level_not_na['level'], level_percentile)  # Top x% threshold\n",
    "    lower_level_threshold = np.percentile(df_level_not_na['level'], 100 - level_percentile)  # Bottom x% threshold\n",
    "\n",
    "    # Identify the outliers: those that are above the upper threshold or below the lower threshold\n",
    "    outliers = df_level_not_na[(df_level_not_na['level'] >= upper_level_threshold) | (df_level_not_na['level'] <= lower_level_threshold)]\n",
    "\n",
    "    # Print the thresholds and outliers for verification\n",
    "    print(f\"Upper level threshold (top {level_percentile}%): {upper_level_threshold}\")\n",
    "    print(f\"Lower level threshold (bottom {100 - level_percentile}%): {lower_level_threshold}\")\n",
    "    print(\"Level Outliers:\")\n",
    "    print(outliers)\n",
    "\n",
    "    # Drop the outliers from the original dataframe\n",
    "    df = df.drop(outliers.index)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "545a1b30-f6c7-400d-b3b8-58e8acb9db0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Preprocess the data for t-SNE\n",
    "def preprocess_data_for_tsne(df, numerical_features=[\"level\", \"spec_filled\"], categorical_features=[\"job\", \"map\"]):\n",
    "    \"\"\"\n",
    "    Preprocesses the data for t-SNE:\n",
    "    - Encodes categorical features (e.g., job, map).\n",
    "    - Imputes missing values for numerical features (not filling with 0).\n",
    "    - Normalizes numerical features.\n",
    "    \"\"\"\n",
    "    # One-Hot Encode categorical features (e.g., job, map)\n",
    "    encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "    encoded_cats = encoder.fit_transform(df[categorical_features])\n",
    "    encoded_df = pd.DataFrame(encoded_cats, columns=encoder.get_feature_names_out(categorical_features))\n",
    "\n",
    "    # Impute missing values for numerical columns with the mean (instead of filling with 0)\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    df[numerical_features] = imputer.fit_transform(df[numerical_features])  # Impute missing numerical values\n",
    "\n",
    "    # Normalize numerical features (including spec_filled)\n",
    "    scaler = StandardScaler()\n",
    "    normalized_nums = scaler.fit_transform(df[numerical_features])\n",
    "    normalized_df = pd.DataFrame(normalized_nums, columns=numerical_features)\n",
    "\n",
    "    # Combine numerical and categorical features\n",
    "    combined_df = pd.concat([normalized_df, encoded_df], axis=1)\n",
    "    return combined_df\n",
    "\n",
    "# Step 2: Apply t-SNE to the data\n",
    "def apply_tsne(df, perplexity=30, learning_rate=200, n_iter=1000):\n",
    "    \"\"\"\n",
    "    Apply t-SNE to the dataset and return the 2D results.\n",
    "    \"\"\"\n",
    "    # Preprocess the data\n",
    "    processed_data = preprocess_data_for_tsne(df)\n",
    "\n",
    "    # Apply t-SNE\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, learning_rate=learning_rate, n_iter=n_iter, random_state=42)\n",
    "    tsne_results = tsne.fit_transform(processed_data)\n",
    "\n",
    "    # Add t-SNE results to the DataFrame\n",
    "    df[\"tsne_dim1\"] = tsne_results[:, 0]\n",
    "    df[\"tsne_dim2\"] = tsne_results[:, 1]\n",
    "\n",
    "    return df\n",
    "\n",
    "# Step 3: Identify outliers based on t-SNE results and filter the top 0.1%\n",
    "def filter_outliers_by_tsne(df, top_percentile=99.9):\n",
    "    \"\"\"\n",
    "    Filter out the top n% (e.g., 0.1%) outliers based on t-SNE distance.\n",
    "    \"\"\"\n",
    "    # Using NearestNeighbors to compute distances between points\n",
    "    neighbors = NearestNeighbors(n_neighbors=2)  # Find the closest point (itself + 1 neighbor)\n",
    "    neighbors.fit(df[[\"tsne_dim1\", \"tsne_dim2\"]])\n",
    "    distances, _ = neighbors.kneighbors(df[[\"tsne_dim1\", \"tsne_dim2\"]])\n",
    "\n",
    "    # Calculate the distance from the nearest neighbor\n",
    "    df[\"distance_to_nearest\"] = distances[:, 1]  # distance to the second nearest (ignoring self)\n",
    "\n",
    "    # Calculate the threshold for the top n% outliers\n",
    "    distance_threshold = np.percentile(df[\"distance_to_nearest\"], top_percentile)\n",
    "    print(f\"Filtering out the top {top_percentile}% outliers with distance greater than {distance_threshold}\")\n",
    "\n",
    "    # Identify the outliers (rows that are filtered out)\n",
    "    outliers = df[df[\"distance_to_nearest\"] > distance_threshold]\n",
    "    \n",
    "    # Filter out the outliers from the original df\n",
    "    df_filtered = df[df[\"distance_to_nearest\"] <= distance_threshold]\n",
    "\n",
    "    return df_filtered, outliers\n",
    "\n",
    "# Example usage:\n",
    "def apply_tsne_oulier_drop(df, top_percentile=99.9):\n",
    "    df_with_tsne = apply_tsne(df)  # Apply t-SNE\n",
    "    df_filtered, outliers = filter_outliers_by_tsne(df_with_tsne, top_percentile=99.9)  # Filter out top 0.1% outliers\n",
    "    df = df.drop(outliers.index)\n",
    "    print(\"tsne outliers:\", outliers)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61408715-0c64-4861-bf42-0a61a3c0e908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(file_paths, output_folder):\n",
    "    all_raw_data = []\n",
    "    all_processed_data = []\n",
    "\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        # Collect initial raw data\n",
    "        raw_df = collect_initial_data(file_path)\n",
    "\n",
    "        # Process and filter valid data\n",
    "        processed_df = process_data(raw_df, file_path)\n",
    "\n",
    "        all_raw_data.append(raw_df)\n",
    "        all_processed_data.append(processed_df)\n",
    "\n",
    "    # Combine all raw DataFrames\n",
    "    combined_raw_df = pd.concat(all_raw_data, ignore_index=True)\n",
    "    combined_processed_df = pd.concat(all_processed_data, ignore_index=True)\n",
    "\n",
    "    # level outlier drop\n",
    "    # combined_raw_df = drop_level_outlier(combined_raw_df, level_percentile=99.99)\n",
    "    # combined_processed_df = drop_level_outlier(combined_processed_df, level_percentile=99.999)\n",
    "    \n",
    "    # tsne outlier drop\n",
    "    # combined_raw_df = apply_tsne_oulier_drop(combined_raw_df, top_percentile=99.99)\n",
    "    # combined_processed_df = apply_tsne_oulier_drop(combined_processed_df, top_percentile=99.999)\n",
    "    \n",
    "    # Save the combined DataFrames\n",
    "    combined_raw_output = os.path.join(output_folder, \"df.csv\")\n",
    "    combined_processed_output = os.path.join(output_folder, \"processed_df.csv\")\n",
    "\n",
    "    combined_raw_df.to_csv(combined_raw_output, index=False)\n",
    "    combined_processed_df.to_csv(combined_processed_output, index=False)\n",
    "\n",
    "    print(f\"Combined raw data saved as '{combined_raw_output}'\")\n",
    "    print(f\"Combined processed data saved as '{combined_processed_output}'\")\n",
    "\n",
    "    return combined_raw_df, combined_processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc8f34b2-d9a1-45c7-bce0-8f3ff3148e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved as data/output/1219.csv\n",
      "Processed valid data saved as data/output/1219_processed.csv\n",
      "Data saved as data/output/1218.csv\n",
      "Processed valid data saved as data/output/1218_processed.csv\n",
      "Data saved as data/output/1220.csv\n",
      "Processed valid data saved as data/output/1220_processed.csv\n",
      "Data saved as data/output/1221.csv\n",
      "Processed valid data saved as data/output/1221_processed.csv\n",
      "Data saved as data/output/1222.csv\n",
      "Processed valid data saved as data/output/1222_processed.csv\n",
      "Data saved as data/output/1213.csv\n",
      "Processed valid data saved as data/output/1213_processed.csv\n",
      "Data saved as data/output/1212.csv\n",
      "Processed valid data saved as data/output/1212_processed.csv\n",
      "Data saved as data/output/1211.csv\n",
      "Processed valid data saved as data/output/1211_processed.csv\n",
      "Data saved as data/output/1215.csv\n",
      "Processed valid data saved as data/output/1215_processed.csv\n",
      "Data saved as data/output/1214.csv\n",
      "Processed valid data saved as data/output/1214_processed.csv\n",
      "Data saved as data/output/1216.csv\n",
      "Processed valid data saved as data/output/1216_processed.csv\n",
      "Data saved as data/output/1217.csv\n",
      "Processed valid data saved as data/output/1217_processed.csv\n",
      "Combined raw data saved as 'data/output/df.csv'\n",
      "Combined processed data saved as 'data/output/processed_df.csv'\n"
     ]
    }
   ],
   "source": [
    "# Process files and save results\n",
    "df, processed_df = process_files(file_paths, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97f3c288-eea0-479f-9a77-4333d56184e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>party_id</th>\n",
       "      <th>time</th>\n",
       "      <th>level</th>\n",
       "      <th>job</th>\n",
       "      <th>spec</th>\n",
       "      <th>map</th>\n",
       "      <th>date</th>\n",
       "      <th>valid</th>\n",
       "      <th>valid_spec</th>\n",
       "      <th>spec_by_level</th>\n",
       "      <th>avg_spec_by_level</th>\n",
       "      <th>spec_filled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>오전 12:33</td>\n",
       "      <td>88</td>\n",
       "      <td>허밋</td>\n",
       "      <td>None</td>\n",
       "      <td>붉켄</td>\n",
       "      <td>2024-12-19</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>18.446821</td>\n",
       "      <td>18.446821</td>\n",
       "      <td>1610.267049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>오전 12:33</td>\n",
       "      <td>101</td>\n",
       "      <td>레인저</td>\n",
       "      <td>None</td>\n",
       "      <td>붉켄</td>\n",
       "      <td>2024-12-19</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>22.312474</td>\n",
       "      <td>22.312474</td>\n",
       "      <td>2212.530949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>오전 12:33</td>\n",
       "      <td>90</td>\n",
       "      <td>허밋</td>\n",
       "      <td>None</td>\n",
       "      <td>붉켄</td>\n",
       "      <td>2024-12-19</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>18.446821</td>\n",
       "      <td>18.446821</td>\n",
       "      <td>1626.593477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>오전 12:33</td>\n",
       "      <td>99</td>\n",
       "      <td>허밋</td>\n",
       "      <td>None</td>\n",
       "      <td>붉켄</td>\n",
       "      <td>2024-12-19</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>18.446821</td>\n",
       "      <td>18.446821</td>\n",
       "      <td>1773.072308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>오전 12:33</td>\n",
       "      <td>105</td>\n",
       "      <td>썬콜</td>\n",
       "      <td>650</td>\n",
       "      <td>붉켄</td>\n",
       "      <td>2024-12-19</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>6.190476</td>\n",
       "      <td>6.212019</td>\n",
       "      <td>650.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3930</th>\n",
       "      <td>292</td>\n",
       "      <td>오후 1:59</td>\n",
       "      <td>143</td>\n",
       "      <td>용기사</td>\n",
       "      <td>5000</td>\n",
       "      <td>블와둥</td>\n",
       "      <td>2024-12-17</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>34.965035</td>\n",
       "      <td>35.282984</td>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3931</th>\n",
       "      <td>297</td>\n",
       "      <td>오후 1:59</td>\n",
       "      <td>119</td>\n",
       "      <td>프리스트</td>\n",
       "      <td>None</td>\n",
       "      <td>검켄</td>\n",
       "      <td>2024-12-17</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>6.320755</td>\n",
       "      <td>6.320755</td>\n",
       "      <td>732.841151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3932</th>\n",
       "      <td>297</td>\n",
       "      <td>오후 1:59</td>\n",
       "      <td>134</td>\n",
       "      <td>프리스트</td>\n",
       "      <td>None</td>\n",
       "      <td>검켄</td>\n",
       "      <td>2024-12-17</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>6.320755</td>\n",
       "      <td>6.320755</td>\n",
       "      <td>841.085808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3933</th>\n",
       "      <td>300</td>\n",
       "      <td>오후 1:59</td>\n",
       "      <td>123</td>\n",
       "      <td>프리스트</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-12-17</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>6.320755</td>\n",
       "      <td>6.320755</td>\n",
       "      <td>784.703748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3934</th>\n",
       "      <td>301</td>\n",
       "      <td>오후 1:59</td>\n",
       "      <td>112</td>\n",
       "      <td>허밋</td>\n",
       "      <td>None</td>\n",
       "      <td>검켄</td>\n",
       "      <td>2024-12-17</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>18.956725</td>\n",
       "      <td>18.956725</td>\n",
       "      <td>2144.175235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3935 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     party_id      time level   job  spec   map        date  valid  \\\n",
       "0           1  오전 12:33    88    허밋  None    붉켄  2024-12-19   True   \n",
       "1           1  오전 12:33   101   레인저  None    붉켄  2024-12-19   True   \n",
       "2           2  오전 12:33    90    허밋  None    붉켄  2024-12-19   True   \n",
       "3           2  오전 12:33    99    허밋  None    붉켄  2024-12-19   True   \n",
       "4           2  오전 12:33   105    썬콜   650    붉켄  2024-12-19   True   \n",
       "...       ...       ...   ...   ...   ...   ...         ...    ...   \n",
       "3930      292   오후 1:59   143   용기사  5000   블와둥  2024-12-17   True   \n",
       "3931      297   오후 1:59   119  프리스트  None    검켄  2024-12-17   True   \n",
       "3932      297   오후 1:59   134  프리스트  None    검켄  2024-12-17   True   \n",
       "3933      300   오후 1:59   123  프리스트  None  None  2024-12-17  False   \n",
       "3934      301   오후 1:59   112    허밋  None    검켄  2024-12-17   True   \n",
       "\n",
       "      valid_spec  spec_by_level  avg_spec_by_level  spec_filled  \n",
       "0          False      18.446821          18.446821  1610.267049  \n",
       "1          False      22.312474          22.312474  2212.530949  \n",
       "2          False      18.446821          18.446821  1626.593477  \n",
       "3          False      18.446821          18.446821  1773.072308  \n",
       "4           True       6.190476           6.212019   650.000000  \n",
       "...          ...            ...                ...          ...  \n",
       "3930        True      34.965035          35.282984  5000.000000  \n",
       "3931       False       6.320755           6.320755   732.841151  \n",
       "3932       False       6.320755           6.320755   841.085808  \n",
       "3933       False       6.320755           6.320755   784.703748  \n",
       "3934       False      18.956725          18.956725  2144.175235  \n",
       "\n",
       "[3935 rows x 12 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74f962f2-afae-46d9-a34d-96daded7dd38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>party_id</th>\n",
       "      <th>time</th>\n",
       "      <th>level</th>\n",
       "      <th>job</th>\n",
       "      <th>spec</th>\n",
       "      <th>map</th>\n",
       "      <th>date</th>\n",
       "      <th>valid</th>\n",
       "      <th>valid_spec</th>\n",
       "      <th>spec_by_level</th>\n",
       "      <th>avg_spec_by_level</th>\n",
       "      <th>spec_filled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>오전 12:33</td>\n",
       "      <td>105</td>\n",
       "      <td>썬콜</td>\n",
       "      <td>650</td>\n",
       "      <td>붉켄</td>\n",
       "      <td>2024-12-19</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>6.190476</td>\n",
       "      <td>6.212019</td>\n",
       "      <td>650.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>오전 12:33</td>\n",
       "      <td>118</td>\n",
       "      <td>썬콜</td>\n",
       "      <td>726</td>\n",
       "      <td>붉켄</td>\n",
       "      <td>2024-12-19</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>6.152542</td>\n",
       "      <td>6.212019</td>\n",
       "      <td>726.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>오전 12:34</td>\n",
       "      <td>128</td>\n",
       "      <td>불독</td>\n",
       "      <td>795</td>\n",
       "      <td>협동</td>\n",
       "      <td>2024-12-19</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>6.210938</td>\n",
       "      <td>6.210938</td>\n",
       "      <td>795.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>오전 12:34</td>\n",
       "      <td>124</td>\n",
       "      <td>용기사</td>\n",
       "      <td>4200</td>\n",
       "      <td>블와둥</td>\n",
       "      <td>2024-12-19</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>33.870968</td>\n",
       "      <td>34.726694</td>\n",
       "      <td>4200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34</td>\n",
       "      <td>오전 12:35</td>\n",
       "      <td>144</td>\n",
       "      <td>허밋</td>\n",
       "      <td>2652</td>\n",
       "      <td>망용둥</td>\n",
       "      <td>2024-12-19</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>18.416667</td>\n",
       "      <td>18.446821</td>\n",
       "      <td>2652.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>260</td>\n",
       "      <td>오후 1:57</td>\n",
       "      <td>106</td>\n",
       "      <td>나이트</td>\n",
       "      <td>3143</td>\n",
       "      <td>듀파</td>\n",
       "      <td>2024-12-17</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>29.650943</td>\n",
       "      <td>30.555809</td>\n",
       "      <td>3143.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>260</td>\n",
       "      <td>오후 1:57</td>\n",
       "      <td>89</td>\n",
       "      <td>나이트</td>\n",
       "      <td>2800</td>\n",
       "      <td>듀파</td>\n",
       "      <td>2024-12-17</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>31.460674</td>\n",
       "      <td>30.555809</td>\n",
       "      <td>2800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>263</td>\n",
       "      <td>오후 1:57</td>\n",
       "      <td>106</td>\n",
       "      <td>프리스트</td>\n",
       "      <td>670</td>\n",
       "      <td>불어전</td>\n",
       "      <td>2024-12-17</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>6.320755</td>\n",
       "      <td>6.320755</td>\n",
       "      <td>670.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>263</td>\n",
       "      <td>오후 1:57</td>\n",
       "      <td>107</td>\n",
       "      <td>용기사</td>\n",
       "      <td>3400</td>\n",
       "      <td>불어전</td>\n",
       "      <td>2024-12-17</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>31.775701</td>\n",
       "      <td>35.282984</td>\n",
       "      <td>3400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>292</td>\n",
       "      <td>오후 1:59</td>\n",
       "      <td>143</td>\n",
       "      <td>용기사</td>\n",
       "      <td>5000</td>\n",
       "      <td>블와둥</td>\n",
       "      <td>2024-12-17</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>34.965035</td>\n",
       "      <td>35.282984</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>458 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    party_id      time level   job  spec  map        date valid  valid_spec  \\\n",
       "0          2  오전 12:33   105    썬콜   650   붉켄  2024-12-19  True        True   \n",
       "1          2  오전 12:33   118    썬콜   726   붉켄  2024-12-19  True        True   \n",
       "2         13  오전 12:34   128    불독   795   협동  2024-12-19  True        True   \n",
       "3         18  오전 12:34   124   용기사  4200  블와둥  2024-12-19  True        True   \n",
       "4         34  오전 12:35   144    허밋  2652  망용둥  2024-12-19  True        True   \n",
       "..       ...       ...   ...   ...   ...  ...         ...   ...         ...   \n",
       "453      260   오후 1:57   106   나이트  3143   듀파  2024-12-17  True        True   \n",
       "454      260   오후 1:57    89   나이트  2800   듀파  2024-12-17  True        True   \n",
       "455      263   오후 1:57   106  프리스트   670  불어전  2024-12-17  True        True   \n",
       "456      263   오후 1:57   107   용기사  3400  불어전  2024-12-17  True        True   \n",
       "457      292   오후 1:59   143   용기사  5000  블와둥  2024-12-17  True        True   \n",
       "\n",
       "     spec_by_level  avg_spec_by_level  spec_filled  \n",
       "0         6.190476           6.212019        650.0  \n",
       "1         6.152542           6.212019        726.0  \n",
       "2         6.210938           6.210938        795.0  \n",
       "3        33.870968          34.726694       4200.0  \n",
       "4        18.416667          18.446821       2652.0  \n",
       "..             ...                ...          ...  \n",
       "453      29.650943          30.555809       3143.0  \n",
       "454      31.460674          30.555809       2800.0  \n",
       "455       6.320755           6.320755        670.0  \n",
       "456      31.775701          35.282984       3400.0  \n",
       "457      34.965035          35.282984       5000.0  \n",
       "\n",
       "[458 rows x 12 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bef677-eeaf-47f1-92ee-34d887387d38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bf38b8-b3d6-4a1a-93f1-aaf2faaeb254",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
