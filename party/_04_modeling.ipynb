{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4894b027-b35c-4336-a810-78b109c143e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "from matplotlib import rc\n",
    "rc('font', family='AppleGothic')\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ec243b0d-bc43-4f2e-a7fc-0a9c5056de97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/output/df.csv\")\n",
    "processed_df = pd.read_csv(\"data/output/processed_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a3bdce6c-e545-4b28-bbb5-3507132d5c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_map_knn(level, job, level_range=5):\n",
    "    # Data Preprocessing: Remove rows with missing values in important columns\n",
    "    df_cleaned = df.dropna(subset=['level', 'spec_filled', 'map'])\n",
    "    \n",
    "    # Filter the dataset to use only the data corresponding to the input job\n",
    "    relevant_data = df_cleaned[(df_cleaned['level'] >= level - level_range) & \n",
    "                               (df_cleaned['level'] <= level + level_range) & \n",
    "                               (df_cleaned['job'] == job)]\n",
    "    \n",
    "    if relevant_data.empty:\n",
    "        return \"No data available for this job/level combination within the specified range.\"\n",
    "\n",
    "    # Prepare feature matrix (X) for KNN model: 'level', 'spec_filled'\n",
    "    X = relevant_data[['level', 'spec_filled']].values\n",
    "    maps_relevant = relevant_data['map'].values\n",
    "\n",
    "    # KNN Model for finding similar maps based on features\n",
    "    knn_relevant = NearestNeighbors(n_neighbors=3, algorithm='auto', metric='euclidean')\n",
    "    knn_relevant.fit(X)\n",
    "\n",
    "    # Create a feature vector for the input (level, spec_filled)\n",
    "    input_spec_filled = relevant_data['spec_filled'].mean()  # Use the mean spec_filled value for input\n",
    "    input_vector = np.array([[level, input_spec_filled]])\n",
    "\n",
    "    # Find the top 3 nearest neighbors (maps) based on the input features\n",
    "    distances, indices = knn_relevant.kneighbors(input_vector)\n",
    "    \n",
    "    # Get the top 3 map recommendations and their corresponding distances\n",
    "    recommended_maps = maps_relevant[indices.flatten()]\n",
    "    recommendation_scores = distances.flatten()\n",
    "\n",
    "    # Remove duplicate maps if any\n",
    "    recommended_maps = list(dict.fromkeys(recommended_maps))[:3]  # Limit to 3 unique maps\n",
    "\n",
    "    # Sort the recommendations by raw distance (lower distance is better)\n",
    "    sorted_recommendations = sorted(zip(recommended_maps, recommendation_scores), key=lambda x: x[1])\n",
    "\n",
    "    # Visualize the KNN results\n",
    "    if isinstance(recommended_maps, list):\n",
    "        maps = relevant_data['map'].values\n",
    "        levels = relevant_data['level'].values\n",
    "        spec_filled = relevant_data['spec_filled'].values\n",
    "        \n",
    "        # Plot all data points\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.scatter(levels, spec_filled, c='gray', label='Data points', alpha=0.5)\n",
    "\n",
    "        # Highlight the query point\n",
    "        plt.scatter(level, input_spec_filled, c='red', label='Query point', s=100, marker='x')\n",
    "\n",
    "        # Highlight the nearest neighbors\n",
    "        nearest_neighbors = X[indices.flatten()]\n",
    "        plt.scatter(nearest_neighbors[:, 0], nearest_neighbors[:, 1], c='blue', label='Nearest neighbors', s=100, marker='o')\n",
    "\n",
    "        # Labels and title\n",
    "        plt.xlabel('Level')\n",
    "        plt.ylabel('Spec Filled')\n",
    "        plt.title(f\"KNN Visualization for Job: {job} and Level: {level}\")\n",
    "        plt.legend()\n",
    "\n",
    "        # Show plot\n",
    "        plt.show()\n",
    "        \n",
    "        # Output the recommended maps and their scores (raw distance)\n",
    "        # Print the similarity scores\n",
    "        print(\"Similarity scores for recommended maps:\")\n",
    "        for map_name, score in zip(recommended_maps, recommendation_scores):\n",
    "            print(f\"Map: {map_name}, Score: {score}\")\n",
    "        print(\"Recommended Maps and Scores:\", recommended_maps)\n",
    "\n",
    "    return sorted_recommendations\n",
    "\n",
    "# Example usage\n",
    "# recommended_maps = recommend_map_knn(140, \"썬콜\", level_range=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "751e7929-f6bb-49c3-9334-fe60abf8ed80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get map recommendations based on a given level and job\n",
    "def recommend_map_svd(level, job, level_range=5):\n",
    "\n",
    "    # Filter out rows that do not belong to the specified job\n",
    "    df_job_filtered = df[df['job'] == job]\n",
    "\n",
    "    # Data Preprocessing: Remove rows with missing values in important columns\n",
    "    df_cleaned = df_job_filtered.dropna(subset=['level', 'job', 'spec_filled', 'map'])\n",
    "    \n",
    "    # Calculate map usage frequency for each job\n",
    "    map_usage_frequency = df_cleaned.groupby(['job', 'map']).size().reset_index(name='usage_count')\n",
    "    \n",
    "    # Calculate the total number of interactions for each job\n",
    "    job_total_count = df_cleaned.groupby('job').size().reset_index(name='total_count')\n",
    "    \n",
    "    # Merge the usage count with the total count for each job\n",
    "    map_usage_frequency = pd.merge(map_usage_frequency, job_total_count, on='job')\n",
    "    \n",
    "    # Calculate the percentage usage for each map within each job\n",
    "    map_usage_frequency['usage_percentage'] = (map_usage_frequency['usage_count'] / map_usage_frequency['total_count']) * 100\n",
    "    \n",
    "    # Filter out maps used in less than 5% of the total interactions for each job\n",
    "    valid_maps_per_job = map_usage_frequency[map_usage_frequency['usage_percentage'] >= 5]\n",
    "    \n",
    "    # Now filter the original dataframe to only keep rows with maps that meet the usage threshold\n",
    "    df_cleaned_filtered = df_cleaned[df_cleaned['map'].isin(valid_maps_per_job['map'])]\n",
    "    \n",
    "    # Prepare the data for the recommendation system\n",
    "    # Create a user-item interaction matrix where rows = users (party_id), columns = maps\n",
    "    user_map_matrix = df_cleaned_filtered.pivot_table(index='party_id', columns='map', values='spec_filled', aggfunc='mean').fillna(0)\n",
    "    \n",
    "    # Apply Matrix Factorization using TruncatedSVD\n",
    "    svd = TruncatedSVD(n_components=3, random_state=42)  # Using 3 latent factors for this example\n",
    "    matrix_factorized = svd.fit_transform(user_map_matrix)\n",
    "    \n",
    "    # Get the cosine similarity between maps in the reduced space\n",
    "    map_similarity = cosine_similarity(matrix_factorized.T)\n",
    "\n",
    "    # Filter data based on the given level and job\n",
    "    level_range_min = level - level_range\n",
    "    level_range_max = level + level_range\n",
    "    relevant_data = df_cleaned_filtered[(df_cleaned_filtered['level'] >= level_range_min) & \n",
    "                                       (df_cleaned_filtered['level'] <= level_range_max) & \n",
    "                                       (df_cleaned_filtered['job'] == job)]\n",
    "    \n",
    "    if relevant_data.empty:\n",
    "        return \"No data available for this job/level combination within the specified range.\"\n",
    "\n",
    "    # Get the list of maps from the filtered data\n",
    "    maps_relevant = relevant_data['map'].unique()\n",
    "\n",
    "    # Ensure that the maps are present in the user_map_matrix\n",
    "    valid_maps = [map_name for map_name in maps_relevant if map_name in user_map_matrix.columns]\n",
    "    \n",
    "    if not valid_maps:\n",
    "        return \"No valid maps available for recommendation.\"\n",
    "\n",
    "    # Get the index of relevant maps in the reduced matrix (svd.transform)\n",
    "    map_indices = [user_map_matrix.columns.get_loc(map_name) for map_name in valid_maps]\n",
    "    \n",
    "    # Ensure map_indices do not exceed the range of the reduced matrix (which has 3 factors)\n",
    "    map_indices = [index for index in map_indices if index < len(map_similarity)]\n",
    "\n",
    "    # Now calculate similarity scores for each relevant map with all other maps in the reduced matrix\n",
    "    scores = np.mean(map_similarity[map_indices], axis=0)\n",
    "\n",
    "    # Sort the scores and get top 3 recommended maps\n",
    "    recommended_indices = np.argsort(scores)[::-1][:3]\n",
    "    recommended_maps = user_map_matrix.columns[recommended_indices]\n",
    "    \n",
    "    # Visualization of the SVD components and map recommendations\n",
    "    # Visualize the maps in the SVD space (2D for simplicity)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Plot all maps in the reduced space (SVD components)\n",
    "    plt.scatter(matrix_factorized[:, 0], matrix_factorized[:, 1], label=\"Maps\", alpha=0.5, c='gray')\n",
    "    \n",
    "    # Highlight the recommended maps in a different color\n",
    "    for idx in recommended_indices:\n",
    "        plt.scatter(matrix_factorized[idx, 0], matrix_factorized[idx, 1], label=f\"Recommended: {recommended_maps[idx]}\", s=100, edgecolors='black')\n",
    "    \n",
    "    # Add labels for the recommended maps\n",
    "    for idx in recommended_indices:\n",
    "        plt.annotate(recommended_maps[idx], (matrix_factorized[idx, 0], matrix_factorized[idx, 1]), fontsize=12, ha='right', color='black')\n",
    "\n",
    "    # Labeling and formatting the plot\n",
    "    plt.title(f\"SVD Map Visualization for Job: {job} and Level: {level}\")\n",
    "    plt.xlabel(\"SVD Component 1\")\n",
    "    plt.ylabel(\"SVD Component 2\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Print the unique maps after preprocessing\n",
    "    print(\"Unique maps after preprocessing:\")\n",
    "    print(df_cleaned_filtered['map'].unique())\n",
    "\n",
    "    # Print the similarity scores\n",
    "    print(\"Similarity scores for recommended maps:\")\n",
    "    for map_name, score in zip(recommended_maps, scores[recommended_indices]):\n",
    "        print(f\"Map: {map_name}, Score: {score}\")\n",
    "    \n",
    "    # Output the recommended maps\n",
    "    print(\"Recommended Maps:\", recommended_maps)\n",
    "    # Return the top 3 recommended maps\n",
    "    return recommended_maps.tolist()\n",
    "\n",
    "# Example usage\n",
    "# recommended_maps = recommend_map_svd(140, \"썬콜\", level_range=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e70add64-7c8e-4c5a-8b91-bbb05378bb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Preprocess the data for t-SNE\n",
    "def preprocess_data_for_tsne(df, numerical_features=[\"level\", \"spec_filled\"], categorical_features=[\"job\", \"map\"]):\n",
    "    \"\"\"\n",
    "    Preprocesses the data for t-SNE:\n",
    "    - Encodes categorical features (e.g., job, map).\n",
    "    - Imputes missing values for numerical features (not filling with 0).\n",
    "    - Normalizes numerical features.\n",
    "    \"\"\"\n",
    "    # One-Hot Encode categorical features (e.g., job, map)\n",
    "    encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "    encoded_cats = encoder.fit_transform(df[categorical_features])\n",
    "    encoded_df = pd.DataFrame(encoded_cats, columns=encoder.get_feature_names_out(categorical_features))\n",
    "\n",
    "    # Impute missing values for numerical columns with the mean (instead of filling with 0)\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    df[numerical_features] = imputer.fit_transform(df[numerical_features])  # Impute missing numerical values\n",
    "\n",
    "    # Normalize numerical features (including spec_filled)\n",
    "    scaler = StandardScaler()\n",
    "    normalized_nums = scaler.fit_transform(df[numerical_features])\n",
    "    normalized_df = pd.DataFrame(normalized_nums, columns=numerical_features)\n",
    "\n",
    "    # Combine numerical and categorical features\n",
    "    combined_df = pd.concat([normalized_df, encoded_df], axis=1)\n",
    "    return combined_df\n",
    "\n",
    "# Step 2: Apply t-SNE to the data\n",
    "def apply_tsne(df, perplexity=30, learning_rate=200, n_iter=1000):\n",
    "    \"\"\"\n",
    "    Apply t-SNE to the dataset and return the 2D results.\n",
    "    \"\"\"\n",
    "    # Preprocess the data\n",
    "    processed_data = preprocess_data_for_tsne(df)\n",
    "\n",
    "    # Apply t-SNE\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, learning_rate=learning_rate, n_iter=n_iter, random_state=42)\n",
    "    tsne_results = tsne.fit_transform(processed_data)\n",
    "\n",
    "    # Add t-SNE results to the DataFrame\n",
    "    df[\"tsne_dim1\"] = tsne_results[:, 0]\n",
    "    df[\"tsne_dim2\"] = tsne_results[:, 1]\n",
    "\n",
    "    return df\n",
    "\n",
    "# Step 3: Identify outliers based on t-SNE results and filter the top 0.1%\n",
    "def filter_outliers_by_tsne(df, top_percentile=99.9):\n",
    "    \"\"\"\n",
    "    Filter out the top n% (e.g., 0.1%) outliers based on t-SNE distance.\n",
    "    \"\"\"\n",
    "    # Using NearestNeighbors to compute distances between points\n",
    "    neighbors = NearestNeighbors(n_neighbors=2)  # Find the closest point (itself + 1 neighbor)\n",
    "    neighbors.fit(df[[\"tsne_dim1\", \"tsne_dim2\"]])\n",
    "    distances, _ = neighbors.kneighbors(df[[\"tsne_dim1\", \"tsne_dim2\"]])\n",
    "\n",
    "    # Calculate the distance from the nearest neighbor\n",
    "    df[\"distance_to_nearest\"] = distances[:, 1]  # distance to the second nearest (ignoring self)\n",
    "\n",
    "    # Calculate the threshold for the top n% outliers\n",
    "    distance_threshold = np.percentile(df[\"distance_to_nearest\"], top_percentile)\n",
    "    print(f\"Filtering out the top {top_percentile}% outliers with distance greater than {distance_threshold}\")\n",
    "\n",
    "    # Identify the outliers (rows that are filtered out)\n",
    "    outliers = df[df[\"distance_to_nearest\"] > distance_threshold]\n",
    "    \n",
    "    # Filter out the outliers from the original df\n",
    "    df_filtered = df[df[\"distance_to_nearest\"] <= distance_threshold]\n",
    "\n",
    "    return df_filtered, outliers\n",
    "\n",
    "# Step 4: Visualize the t-SNE results\n",
    "def visualize_tsne(df):\n",
    "    \"\"\"\n",
    "    Visualizes the t-SNE results and shows the distribution of job-map combinations.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Create the scatter plot\n",
    "    sns.scatterplot(data=df, x=\"tsne_dim1\", y=\"tsne_dim2\", hue=\"map\", style=\"job\", palette=\"tab20\", s=100, alpha=0.7)\n",
    "    plt.title(\"t-SNE Visualization of Job-Map Combinations\", fontsize=16)\n",
    "    plt.xlabel(\"t-SNE Dimension 1\", fontsize=14)\n",
    "    plt.ylabel(\"t-SNE Dimension 2\", fontsize=14)\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1), title=\"Job / Map\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d23bbd4-7e16-476f-ae80-1adf0da3f1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tsne(df):\n",
    "    df_with_tsne = apply_tsne(df)  # Apply t-SNE\n",
    "    df_filtered, outliers = filter_outliers_by_tsne(df_with_tsne, top_percentile=99.9)\n",
    "    visualize_tsne(df_filtered)\n",
    "    print(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3333266e-4ba7-4d8d-bf07-8c10511152ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommended_maps = recommend_map_knn(140, \"용기사\", level_range=5)\n",
    "# recommended_maps = recommend_map_svd(140, \"용기사\", level_range=5)\n",
    "# process_tsne(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae04314-e32b-42a5-906b-95a8ee99e195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4281c2d-e70a-4b1c-add1-936cde689915",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
