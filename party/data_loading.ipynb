{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e67ee6e9-e19e-4bc9-8326-9e843a6bd2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "#https://discord.com/channels/1134059900666916935/1283610000484208670"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc58157e-c2e3-4ccc-bc8d-24775d41b3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output folders\n",
    "input_folder = \"data/txt\"\n",
    "output_folder = \"data/output\"\n",
    "file_paths = [os.path.join(input_folder, file) for file in os.listdir(input_folder) if file.endswith(\".txt\")]\n",
    "\n",
    "# Ensure the output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Job dictionary and spec ranges\n",
    "job_dict = {\n",
    "    \"허\": \"허밋\", \"시프\": \"시프\", \"썬\": \"썬콜\", \"불독\": \"불독\",\n",
    "    \"프\": \"프리스트\", \"레\": \"레인저\", \"저\": \"저격수\",\n",
    "    \"용\": \"용기사\", \"크\": \"크루세이더\", \"나\": \"나이트\"\n",
    "}\n",
    "job_spec_ranges = {\n",
    "    \"용\": (3000, 9000), \"크\": (2000, 9000), \"나\": (2000, 9000),\n",
    "    \"허\": (1500, 4000), \"시프\": (1500, 5000), \"썬\": (500, 1200),\n",
    "    \"불독\": (500, 1200), \"프\": (500, 1200), \"레\": (2000, 9000),\n",
    "    \"저\": (2000, 9000)\n",
    "}\n",
    "level_min, level_max = 80, 200\n",
    "\n",
    "# Regex patterns\n",
    "time_pattern = r\"(오전|오후) \\d{1,2}:\\d{2}\"\n",
    "job_pattern = r\"(\\d{2,3})\\s?(\" + \"|\".join(job_dict.keys()) + r\")\"\n",
    "spec_pattern = r\"(\\d{3,4})\"\n",
    "map_pattern = r\"(망용둥|위둥|남둥|큰둥|와협|블와둥|협동|레와둥|붉켄|검켄|푸켄|불어전|물어전|오징어|깊바협|망둥쩔|듀파|듀미굴|갈림길|산양|하둥)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e6e0ade-234a-425c-afee-14e3b3707f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_level_and_spec(text, job_start, job_end, level_range, spec_range):\n",
    "    \"\"\"\n",
    "    Extracts level and spec values by searching limited characters around the job.\n",
    "    Level is searched from the left, and spec is searched from the right.\n",
    "    \"\"\"\n",
    "    # Limit the search space\n",
    "    left_text = text[job_start-5 : job_start+5]  # 6 characters left of the job\n",
    "    right_text = text[job_end+1 : job_end + 10]  # 6 characters right of the job\n",
    "\n",
    "    # print (\"left:\", left_text, \"right:\", right_text)\n",
    "    \n",
    "    level, spec = None, None\n",
    "\n",
    "    # Search for level in the left_text\n",
    "    for match in re.finditer(r\"\\d{2,3}\", left_text[::-1]):  # Reverse for easier parsing\n",
    "        num = int(match.group()[::-1])  # Reverse back the number\n",
    "        if level_range[0] <= num <= level_range[1]:\n",
    "            level = num\n",
    "            break\n",
    "\n",
    "    # Search for spec in the right_text\n",
    "    for match in re.finditer(r\"\\d{3,4}\", right_text):\n",
    "        num = int(match.group())\n",
    "        if spec_range[0] <= num <= spec_range[1]:\n",
    "            spec = num\n",
    "            break\n",
    "\n",
    "    return level, spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5cca60f2-0bd3-4ce8-b269-e7ab6eeffafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_initial_data(file_path):\n",
    "    columns = [\"party_id\", \"time\", \"level\", \"job\", \"spec\", \"map\", \"date\", \"valid\", \"valid_spec\"]\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "\n",
    "    # Extract date from filename\n",
    "    month = file_path[-8:-6]\n",
    "    day = file_path[-6:-4]\n",
    "    date_from_file = f\"2024-{month}-{day}\"\n",
    "    \n",
    "    party_id = 1\n",
    "    \n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = file.read()\n",
    "\n",
    "    blocks = data.split(\"오늘\")\n",
    "    for block in blocks[1:]:\n",
    "        time_match = re.search(time_pattern, block)\n",
    "        time = time_match.group(0) if time_match else None\n",
    "        map_match = re.search(map_pattern, block)\n",
    "        map_name = map_match.group(0) if map_match else None\n",
    "\n",
    "        job_matches = list(re.finditer(job_pattern, block))\n",
    "        for job_match in job_matches:\n",
    "            job_short = job_match.group(2)\n",
    "            job_full = job_dict.get(job_short, \"Unknown\")\n",
    "\n",
    "            level, spec = extract_level_and_spec(\n",
    "                block, job_match.start(), job_match.end(),\n",
    "                (level_min, level_max), job_spec_ranges.get(job_short, (None, None))\n",
    "            )\n",
    "            \n",
    "            valid = map_name is not None and job_full != \"Unknown\"\n",
    "\n",
    "            # print(level, job_full, spec)\n",
    "            \n",
    "            # Append new row to DataFrame\n",
    "            df = pd.concat([\n",
    "                df,\n",
    "                pd.DataFrame([{\n",
    "                    \"party_id\": party_id,\n",
    "                    \"time\": time,\n",
    "                    \"level\": level,\n",
    "                    \"job\": job_full,\n",
    "                    \"spec\": spec,\n",
    "                    \"map\": map_name,\n",
    "                    \"date\": date_from_file,\n",
    "                    \"valid\": valid,\n",
    "                    \"valid_spec\": None\n",
    "                }])\n",
    "            ], ignore_index=True)\n",
    "\n",
    "        party_id += 1\n",
    "\n",
    "    # Remove duplicates\n",
    "    df = df.drop_duplicates(subset=[\"map\", \"level\", \"job\", \"spec\"], keep=\"first\")\n",
    "\n",
    "    # Filter Level Outlier\n",
    "    level_percentile=0.02\n",
    "    df_level_not_na = df[df['level'].notna()]\n",
    "    level_threshold = np.percentile(df_level_not_na['level'], level_percentile)\n",
    "    df = df_level_not_na[df_level_not_na['level'] >= level_threshold]\n",
    "    \n",
    "    #스펙 결측치 처리\n",
    "    # Step 1: Calculate spec_by_level, handling NaN values for spec and level\n",
    "    df[\"spec_by_level\"] = df.apply(\n",
    "        lambda row: row[\"spec\"] / row[\"level\"] if pd.notna(row[\"spec\"]) and pd.notna(row[\"level\"]) else None,\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Step 2: Calculate average spec_by_level by job\n",
    "    avg_spec_by_level = df.groupby(\"job\")[\"spec_by_level\"].mean().rename(\"avg_spec_by_level\")\n",
    "    \n",
    "    # Step 3: Merge the avg_spec_by_level back into the dataframe\n",
    "    df = df.merge(avg_spec_by_level, on=\"job\", how=\"left\")\n",
    "    \n",
    "    # Step 4: Fill NaN in spec_by_level with the average spec_by_level for the job\n",
    "    df[\"spec_by_level\"].fillna(df[\"avg_spec_by_level\"], inplace=True)\n",
    "    \n",
    "    # Step 5: Determine spec_valid based on 20% margin from avg_spec_by_level\n",
    "    def is_spec_valid(row):\n",
    "        if pd.notna(row[\"spec\"]):  # Only proceed if spec is not NaN\n",
    "            if pd.notna(row[\"spec_by_level\"]) and pd.notna(row[\"avg_spec_by_level\"]):\n",
    "                lower_bound = 0.8 * row[\"avg_spec_by_level\"]\n",
    "                upper_bound = 1.2 * row[\"avg_spec_by_level\"]\n",
    "                return lower_bound <= row[\"spec_by_level\"] <= upper_bound\n",
    "        return False  # Return False if spec is NaN or not within the range\n",
    "    \n",
    "    # Step 6: Apply the is_spec_valid function to the dataframe\n",
    "    df[\"valid_spec\"] = df.apply(is_spec_valid, axis=1)\n",
    "    \n",
    "    # Step 7: Create spec_filled: if spec is NaN, fill with level * avg_spec_by_level, else keep spec as is\n",
    "    df[\"spec_filled\"] = df.apply(\n",
    "        lambda row: row[\"spec\"] if pd.notna(row[\"spec\"]) else (\n",
    "            row[\"level\"] * row[\"avg_spec_by_level\"] if pd.notna(row[\"level\"]) and pd.notna(row[\"avg_spec_by_level\"]) else None\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Step 8: Calculate the average distance between spec and avg_spec_by_level for each job\n",
    "    df[\"spec_distance\"] = df.apply(\n",
    "        lambda row: abs(row[\"spec\"] - row[\"avg_spec_by_level\"]) if pd.notna(row[\"spec\"]) and pd.notna(row[\"avg_spec_by_level\"]) else None,\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Step 9: Calculate the average distance for each job\n",
    "    job_avg_distances = df.groupby(\"job\")[\"spec_distance\"].mean().rename(\"avg_distance_per_job\")\n",
    "    \n",
    "    # Step 10: Merge the average distance per job back into the dataframe\n",
    "    df = df.merge(job_avg_distances, on=\"job\", how=\"left\")\n",
    "\n",
    "    JITTER_PERCENTAGE = 0.03\n",
    "    # Step 11: Apply jitter based on the average distance per job\n",
    "    # Apply jitter ONLY if spec_filled is created (i.e., spec is NaN)\n",
    "    df[\"spec_filled\"] = df.apply(\n",
    "    lambda row: row[\"spec_filled\"] + np.random.uniform(-row[\"spec_filled\"] * JITTER_PERCENTAGE, row[\"spec_filled\"] * JITTER_PERCENTAGE)\n",
    "    if pd.isna(row[\"spec\"]) and pd.notna(row[\"spec_filled\"]) else row[\"spec_filled\"],  # Apply jitter only when spec is missing\n",
    "    axis=1\n",
    "    )\n",
    "    \n",
    "    output_file = os.path.join(output_folder, os.path.basename(file_path).replace('.txt', '.csv'))\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Data saved as {output_file}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3112bdb-12d8-4c51-866a-beae9b355bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(df, file_path):\n",
    "    \"\"\"\n",
    "    Filters and processes data to save only rows where valid and valid_spec are True.\n",
    "    \"\"\"\n",
    "    # Filter rows where both 'valid' and 'valid_spec' are True\n",
    "    processed_df = df[(df[\"valid\"]) & (df[\"valid_spec\"])]\n",
    "\n",
    "    # Save the processed DataFrame to a CSV file\n",
    "    processed_output_file = os.path.join(output_folder, os.path.basename(file_path).replace('.txt', '_processed.csv'))\n",
    "    processed_df.to_csv(processed_output_file, index=False)\n",
    "\n",
    "    print(f\"Processed valid data saved as {processed_output_file}\")\n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61408715-0c64-4861-bf42-0a61a3c0e908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(file_paths, output_folder):\n",
    "    \"\"\"\n",
    "    Processes multiple text files, combines them into a single DataFrame for valid processed data,\n",
    "    and saves the final output as CSV files.\n",
    "\n",
    "    Parameters:\n",
    "        file_paths (list): List of paths to the text files.\n",
    "        output_folder (str): Path to the folder where CSV files will be saved.\n",
    "    \"\"\"\n",
    "    all_raw_data = []\n",
    "    all_valid_processed_data = []\n",
    "\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        # Collect initial raw data\n",
    "        raw_df = collect_initial_data(file_path)\n",
    "\n",
    "        # Process and filter valid data\n",
    "        valid_processed_df = process_data(raw_df, file_path)\n",
    "\n",
    "        all_raw_data.append(raw_df)\n",
    "        all_valid_processed_data.append(valid_processed_df)\n",
    "\n",
    "    # Combine all raw DataFrames\n",
    "    combined_raw_df = pd.concat(all_raw_data, ignore_index=True)\n",
    "    combined_valid_processed_df = pd.concat(all_valid_processed_data, ignore_index=True)\n",
    "\n",
    "    # Save the combined DataFrames\n",
    "    combined_raw_output = os.path.join(output_folder, \"df.csv\")\n",
    "    combined_processed_output = os.path.join(output_folder, \"processed_df.csv\")\n",
    "\n",
    "    combined_raw_df.to_csv(combined_raw_output, index=False)\n",
    "    combined_valid_processed_df.to_csv(combined_processed_output, index=False)\n",
    "\n",
    "    print(f\"Combined raw data saved as '{combined_raw_output}'\")\n",
    "    print(f\"Combined valid processed data saved as '{combined_processed_output}'\")\n",
    "\n",
    "    return combined_raw_df, combined_valid_processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc8f34b2-d9a1-45c7-bce0-8f3ff3148e07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved as data/output/1219.csv\n",
      "Processed valid data saved as data/output/1219_processed.csv\n",
      "Data saved as data/output/1218.csv\n",
      "Processed valid data saved as data/output/1218_processed.csv\n",
      "Data saved as data/output/1220.csv\n",
      "Processed valid data saved as data/output/1220_processed.csv\n",
      "Data saved as data/output/1221.csv\n",
      "Processed valid data saved as data/output/1221_processed.csv\n",
      "Data saved as data/output/1213.csv\n",
      "Processed valid data saved as data/output/1213_processed.csv\n",
      "Data saved as data/output/1212.csv\n",
      "Processed valid data saved as data/output/1212_processed.csv\n",
      "Data saved as data/output/1211.csv\n",
      "Processed valid data saved as data/output/1211_processed.csv\n",
      "Data saved as data/output/1215.csv\n",
      "Processed valid data saved as data/output/1215_processed.csv\n",
      "Data saved as data/output/1214.csv\n",
      "Processed valid data saved as data/output/1214_processed.csv\n",
      "Data saved as data/output/1216.csv\n",
      "Processed valid data saved as data/output/1216_processed.csv\n",
      "Data saved as data/output/1217.csv\n",
      "Processed valid data saved as data/output/1217_processed.csv\n",
      "Combined raw data saved as 'data/output/df.csv'\n",
      "Combined valid processed data saved as 'data/output/processed_df.csv'\n"
     ]
    }
   ],
   "source": [
    "# Process files and save results\n",
    "df, processed_df = process_files(file_paths, output_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
